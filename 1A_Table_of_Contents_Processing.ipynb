{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjT36+tTng8BbL9aupCd6x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Noelle-Pastor/UTA-Libraries-Data-Visualization-Contest/blob/main/1A_Table_of_Contents_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI Agent: Table of Contents (ToC) Processing\n",
        "## **.JPG** ---> **.CSV**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### **Input:**\n",
        "- Set of photos from ToC\n",
        "\n",
        "\n",
        "### **Output:**\n",
        "- One .CSV representing ToC structure\n",
        "- One .CSV containing ToC metadata"
      ],
      "metadata": {
        "id": "2ovXOoMsWHLq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "DumufSJ2AMkT"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "from pathlib import Path\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create dynamic, detailed prompt. This prompt:\n",
        "- Specifies rules for processing the images into structured format\n",
        "- Outputs response in a .json format\n",
        "- Works in batches; dynamically passes in last line from previous batch"
      ],
      "metadata": {
        "id": "KzjaQ_rYYYCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_prompt(last_hierarchy, last_metadata):\n",
        "\n",
        "    context_block = f\"\"\" CONTEXT FROM PREVIOUS BATCH (Optional):\n",
        "    •\tInstructions: If this block is provided, use its values as the starting state for your analysis of the new images. Increment the last ID by 1. The first items on the new page may be children of these context entries. If this block's values are empty, start from a clean slate.\n",
        "\n",
        "    •\tLast Hierarchy Context:\n",
        "    o\tLast ID: \"{last_hierarchy[\"ID\"]}\"\n",
        "    o\tHeading 1: \"{last_hierarchy[\"Heading 1\"]}\"\n",
        "    o\tHeading 2: \"{last_hierarchy[\"Heading 2\"]}\"\n",
        "    o\tHeading 3: \"{last_hierarchy[\"Heading 3\"]}\"\n",
        "    o\tHeading 4: \"{last_hierarchy[\"Heading 4\"]}\"\n",
        "\n",
        "    •\tLast Metadata Context:\n",
        "    o\tAnthology Title: \"{last_metadata[\"Anthology Title\"]}\"\n",
        "    o\tPublish Date: \"{last_metadata[\"Publish Date\"]}\"\n",
        "    o\tAuthor 1: \"{last_metadata[\"Author 1\"]}\"\n",
        "    o\tAuthor 2: \"{last_metadata[\"Author 2\"]}\"\n",
        "    o\tAuthor 3: \"{last_metadata[\"Author 3\"]}\"\n",
        "    o\tPates: \"{last_metadata[\"Pages\"]}\"\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    json_specification = \"\"\"\n",
        "    {{\n",
        "      \"HIERARCHY\": [\n",
        "        {{\"ID\": 1, \"Page Number\": \"...\", \"Heading 1\": \"...\", \"Heading 2\": \"...\", \"Heading 3\": \"...\", \"Heading 4\": \"...\", \"Heading 5\": \"...\"}\n",
        "    }\n",
        "      ],\n",
        "\n",
        "      \"METADATA\": [\n",
        "        {{\"Anthology Title\": \"...\", \"Publish Date\": \"...\", \"Author 1\": \"...\", \"Author 2\": \"...\", \"Author 3\": \"...\", \"Pages\": \"...\"}\n",
        "    }\n",
        "      ]\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    json_example = \"\"\"\n",
        "    {\n",
        "      \"HIERARCHY\": [\n",
        "        {\n",
        "          \"ID\": 1,\n",
        "          \"Page Number\": \"1\",\n",
        "          \"Heading 1\": \"PART 1: THE EARLY YEARS\",\n",
        "          \"Heading 2\": \"\",\n",
        "          \"Heading 3\": \"\",\n",
        "          \"Heading 4\": \"\",\n",
        "          \"Heading 5\": \"\"\n",
        "        },\n",
        "        {\n",
        "          \"ID\": 2,\n",
        "          \"Page Number\": \"5\",\n",
        "          \"Heading 1\": \"PART 1: THE EARLY YEARS\",\n",
        "          \"Heading 2\": \"Colonial Voices\",\n",
        "         \"Heading 3\": \"\",\n",
        "          \"Heading 4\": \"\",\n",
        "          \"Heading 5\": \"\"\n",
        "        },\n",
        "        {\n",
        "          \"ID\": 3,\n",
        "          \"Page Number\": \"7\",\n",
        "          \"Heading 1\": \"PART 1: THE EARLY YEARS\",\n",
        "          \"Heading 2\": \"Colonial Voices\",\n",
        "          \"Heading 3\": \"Anne Bradstreet (1612-1672)\",\n",
        "          \"Heading 4\": \"\",\n",
        "          \"Heading 5\": \"\"\n",
        "        },\n",
        "        {\n",
        "          \"ID\": 4,\n",
        "          \"Page Number\": \"8\",\n",
        "          \"Heading 1\": \"PART 1: THE EARLY YEARS\",\n",
        "          \"Heading 2\": \"Colonial Voices\",\n",
        "          \"Heading 3\": \"Anne Bradstreet (1612-1672)\",\n",
        "          \"Heading 4\": \"The Prologue\",\n",
        "          \"Heading 5\": \"\"\n",
        "        },\n",
        "        {\n",
        "          \"ID\": 5,\n",
        "          \"Page Number\": \"9\",\n",
        "          \"Heading 1\": \"PART 1: THE EARLY YEARS\",\n",
        "          \"Heading 2\": \"Colonial Voices\",\n",
        "          \"Heading 3\": \"Anne Bradstreet (1612-1672)\",\n",
        "          \"Heading 4\": \"To My Dear and Loving Husband\",\n",
        "          \"Heading 5\": \"\"\n",
        "        }\n",
        "      ],\n",
        "      \"METADATA\": [\n",
        "        {\n",
        "          \"Anthology Title\": \"American Literature: A Survey\",\n",
        "          \"Publish Date\": \"2023\",\n",
        "          \"Author 1\": \"Jane Doe\",\n",
        "          \"Author 2\": \"John Smith\",\n",
        "          \"Author 3\": \"\",\n",
        "          \"Pages\": \"9\"\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Objective:\n",
        "\n",
        "    You are an expert data extraction and interpretation agent. You will be given a batch of images representing a book's title page and/or its table of contents (ToC).\n",
        "    Your task is to analyze these images and generate a single, valid JSON object containing three distinct data structures derived from the content.\n",
        "    Analyze the series of images containing a book's title page and its complete table of contents (ToC).\n",
        "    From this single set of images, you will generate a single, complete JSON object that contains all the extracted data, adhering strictly to \\the specified structure and logic.\n",
        "\n",
        "\n",
        "    Input:\n",
        "    A sequence of images. The first image will be the book's title page. The subsequent images will be the pages of the table of contents. Treat all ToC images as a single, continuous document.\n",
        "\n",
        "    Output:\n",
        "    You must generate a single, valid JSON object and nothing else. The JSON object must have two top-level keys: HIERARCHY and METADATA. The value for each key must be a list of JSON objects, where each object represents one row of data. It is critical that every key is present in every object, using an empty string \"\" for any field that has no value. Do not use null.\n",
        "\n",
        "\n",
        "    JSON Structure Specification (generate nothing outside of the first and last curly braces. DO NOT include the word \"json\" before the file/text.):\n",
        "    {json_specification}\n",
        "\n",
        "    BATCHES:\n",
        "    The complete JSON file will be created in multiple batches.\n",
        "\n",
        "\n",
        "    Core Logic & Rules:\n",
        "    {context_block}\n",
        "\n",
        "\n",
        "    1.\tGeneral Processing Rules (Apply to all data extraction)\n",
        "    •\tUTF-8 Encoding: Ensure all text output uses UTF-8 encoding to correctly represent all characters.\n",
        "    •\tIgnore Irrelevant Information: Actively ignore all text and graphics not required for the output, including publisher data, ISBNs, dedications, and page footers.\n",
        "    •\tMulti-line Entries: If a single conceptual entry (like a long title) is visually broken across multiple lines, intelligently merge them into a single field.\n",
        "\n",
        "    2.\tInitial Analysis & Metadata (for the METADATA key):\n",
        "    •\tFirst, analyze the title page image to extract the \"Anthology Title,\" \"Publish Date,\" and up to three primary authors.\n",
        "    •\tActively ignore all other text on the pages (publisher info, ISBN, etc.).\n",
        "    •\tDetermine the total \"Pages\" by finding the last numerical page number in the main body of the ToC.\n",
        "    •\tFormat this information as a list containing a single JSON object.\n",
        "    •\tEnsure all text output uses UTF-8 encoding to correctly represent all characters.\n",
        "\n",
        "    3.\tHierarchical Extraction (for the HIERARCHY key):\n",
        "    •\tProcess the ToC images to capture their literal structure.\n",
        "    •\tHandle multi-line entries: If a single entry is visually broken across multiple lines, intelligently merge them into a single field.\n",
        "    •\tFor each line item, create a new JSON object. Assign a sequential integer ID.\n",
        "    •\tExtract the Page Number or the word \"online\".\n",
        "    •\tDetermine the hierarchical level (1-5) based on visual cues (indentation, font styles).\n",
        "    •\tPopulate the Heading 1 through Heading 5 fields. Parent headings must be populated.\n",
        "\n",
        "\n",
        "    START OF EXAMPLE\n",
        "    To be clear, here is a perfect example of how to perform this entire task.\n",
        "\n",
        "    Example Input:\n",
        "    (Imagine one image of a title page and one image of a ToC are provided)\n",
        "\n",
        "    Example Output:\n",
        "    {json_example}\n",
        "\n",
        "    END OF EXAMPLE\n",
        "\n",
        "    Now, apply this exact logic and JSON structure to the new images I will provide. Produce a single, valid JSON object as the complete output.\"\n",
        "    \"\"\"\n",
        "\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "dLZ2eGnPYRZN"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure response is clean json"
      ],
      "metadata": {
        "id": "oK4XLCzTZWmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_response(s):\n",
        "    s = s.strip().removeprefix(\"```json\").removesuffix(\"```\").strip()\n",
        "    s.replace('\\n', '\\\\n').replace('\\r', '\\\\r').replace('\\t', '\\\\t')\n",
        "    return s"
      ],
      "metadata": {
        "id": "ZWtruvhlZUPz"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create client"
      ],
      "metadata": {
        "id": "6pg73uokZlLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GEMINI_API_KEY = \"\"\n",
        "client = genai.Client(api_key = GEMINI_API_KEY)"
      ],
      "metadata": {
        "id": "X2CCwRkMZc6f"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set batch size"
      ],
      "metadata": {
        "id": "qhx11Ix3Z8zz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10"
      ],
      "metadata": {
        "id": "UIwFR1JiZ38N"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Path object; path to directory of directories, that each contain the images for a single ToC"
      ],
      "metadata": {
        "id": "5Rd9oRw7Z676"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = Path(\"insert_path_here\")"
      ],
      "metadata": {
        "id": "fYRK_AC0aLdP"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get list of names of all folders in directory"
      ],
      "metadata": {
        "id": "sBk2r3gCabE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "toc_titles = os.listdir(path)"
      ],
      "metadata": {
        "id": "sLwWvhpYa1K7"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9UItxJOibu_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for each ToC folder\n",
        "for folder_path, title in zip(path.iterdir(), toc_titles):\n",
        "\n",
        "\n",
        "    # skip hidden files like \".DS_Store\"\n",
        "    if not folder_path.is_dir():\n",
        "        continue\n",
        "\n",
        "\n",
        "    # create temporary lists; add to them each batch to create .csv's at end\n",
        "    HIERARCHY_alldata = []\n",
        "    METADATA_alldata = []\n",
        "\n",
        "\n",
        "    #reset batch memory for new ToC\n",
        "    last_hierarchy = {\"ID\":\"\", \"Heading 1\":\"\", \"Heading 2\":\"\", \"Heading 3\":\"\", \"Heading 4\":\"\"}\n",
        "    last_metadata = {\"Anthology Title\": \"\", \"Publish Date\": \"\", \"Author 1\": \"\", \"Author 2\": \"\", \"Author 3\": \"\", \"Pages\": \"\"}\n",
        "\n",
        "\n",
        "\n",
        "    # Get a list of the photo filepaths  (then remove \".DS_Store\" hidden file)\n",
        "    photo_paths = sorted(list(folder_path.iterdir()))\n",
        "    photo_paths = photo_paths[1:]\n",
        "\n",
        "\n",
        "    for i in range(0, len(photo_paths), batch_size):\n",
        "        print(f\"\\nBatch {i//batch_size+1}:\")\n",
        "\n",
        "\n",
        "        #slice to get filepaths for photos in batch\n",
        "        batch_paths = photo_paths[i : i + batch_size]\n",
        "\n",
        "\n",
        "\n",
        "        # Upload the photos in batch to api; make uploaded photos list\n",
        "        uploaded_photos = []\n",
        "        for path in batch_paths:\n",
        "\n",
        "\n",
        "            # From Google's documentation on passing images\n",
        "            photo = client.files.upload(file=path)\n",
        "            print(f\"{len(uploaded_photos)+1} photos uploaded\")\n",
        "\n",
        "\n",
        "            #add to list of uploaded photo objects\n",
        "            uploaded_photos.append(photo)\n",
        "\n",
        "            #make sure not too many requests at once\n",
        "            time.sleep(1)\n",
        "\n",
        "\n",
        "        #pass prompt and photos to model and get response\n",
        "        response = client.models.generate_content(\n",
        "        model=\"gemini-flash-latest\",\n",
        "        contents=[\n",
        "\n",
        "            create_prompt(last_hierarchy, last_metadata),\n",
        "            *uploaded_photos\n",
        "\n",
        "          ])\n",
        "\n",
        "\n",
        "        #clear uploaded files\n",
        "        for file in uploaded_photos:\n",
        "            client.files.delete(name=file.name)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        cleaned_string = clean_response(response.text)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Creates dictionary from json keys\n",
        "        data = json.loads(cleaned_string)\n",
        "\n",
        "\n",
        "        # Add the new rows to lists\n",
        "        HIERARCHY_alldata.extend(data['HIERARCHY'])\n",
        "        METADATA_alldata.extend(data['METADATA'])\n",
        "        print(\"Batch appended.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #update context block in prompt (allows for batching, remembering the last line of previous batch)\n",
        "        if HIERARCHY_alldata:\n",
        "            last_hierarchy[\"ID\"] = HIERARCHY_alldata[-1][\"ID\"]\n",
        "            last_hierarchy[\"Heading 1\"] = HIERARCHY_alldata[-1][\"Heading 1\"]\n",
        "            last_hierarchy[\"Heading 2\"] = HIERARCHY_alldata[-1][\"Heading 2\"]\n",
        "            last_hierarchy[\"Heading 3\"] = HIERARCHY_alldata[-1][\"Heading 3\"]\n",
        "            last_hierarchy[\"Heading 4\"] = HIERARCHY_alldata[-1][\"Heading 4\"]\n",
        "            print(\"Batch memory data updated\")\n",
        "\n",
        "        if METADATA_alldata:\n",
        "            last_metadata[\"Anthology Title\"] = METADATA_alldata[-1][\"Anthology Title\"]\n",
        "            last_metadata[\"Publish Date\"] = METADATA_alldata[-1][\"Publish Date\"]\n",
        "            last_metadata[\"Author 1\"] = METADATA_alldata[-1][\"Author 1\"]\n",
        "            last_metadata[\"Author 2\"] = METADATA_alldata[-1][\"Author 2\"]\n",
        "            last_metadata[\"Author 3\"] = METADATA_alldata[-1][\"Author 3\"]\n",
        "            last_metadata[\"Pages\"] = METADATA_alldata[-1][\"Pages\"]\n",
        "            print(\"Batch memory data updated\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # List to DataFrame\n",
        "    hierarchy_df = pd.DataFrame(HIERARCHY_alldata)\n",
        "    metadata_df = pd.DataFrame(METADATA_alldata)\n",
        "\n",
        "    # DataFrame to .csv\n",
        "    hierarchy_df.to_csv(f\"{title}_HIERARCHY.csv\", sep= \"\\t\", index=False)\n",
        "    metadata_df.to_csv(f\"{title}_METADATA.csv\", sep= \"\\t\", index=False)\n",
        "    print(\"CSV's saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MpJjDd-6bqCt",
        "outputId": "41bca8dd-b56c-42fb-a357-3d8928ea3319"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Batch 1\n",
            "1 photos uploaded\n",
            "2 photos uploaded\n",
            "3 photos uploaded\n",
            "4 photos uploaded\n",
            "5 photos uploaded\n",
            "6 photos uploaded\n",
            "7 photos uploaded\n",
            "8 photos uploaded\n",
            "9 photos uploaded\n",
            "10 photos uploaded\n",
            "Batch appended.\n",
            "Batch memory data updated\n",
            "Batch memory data updated\n",
            "\n",
            "Batch 11\n",
            "1 photos uploaded\n",
            "2 photos uploaded\n",
            "3 photos uploaded\n",
            "4 photos uploaded\n",
            "5 photos uploaded\n",
            "6 photos uploaded\n",
            "7 photos uploaded\n",
            "8 photos uploaded\n",
            "9 photos uploaded\n",
            "10 photos uploaded\n",
            "Batch appended.\n",
            "Batch memory data updated\n",
            "Batch memory data updated\n",
            "\n",
            "Batch 21\n",
            "1 photos uploaded\n",
            "2 photos uploaded\n",
            "3 photos uploaded\n",
            "4 photos uploaded\n",
            "5 photos uploaded\n",
            "6 photos uploaded\n",
            "7 photos uploaded\n",
            "8 photos uploaded\n",
            "9 photos uploaded\n",
            "10 photos uploaded\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "JSONDecodeError",
          "evalue": "Invalid control character at: line 3500 column 16 (char 106915)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2561230234.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# Creates dictionary from json keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \"\"\"\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    352\u001b[0m         \"\"\"\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Invalid control character at: line 3500 column 16 (char 106915)"
          ]
        }
      ]
    }
  ]
}